{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(url):\n",
    "    timeout = 60\n",
    "    while True:  # Keep trying until the webpage successfully downloads\n",
    "        try:\n",
    "            web_input = requests.get(url, stream=True, timeout=timeout)\n",
    "            break  # If it downloads, get out and get on with life\n",
    "        except requests.exceptions.RequestException as e:  # If it doesn't download after the timeout period, an exceptions is thrown, and we try again\n",
    "            print(e)\n",
    "            pass\n",
    "    soup = BeautifulSoup(web_input.content, 'html.parser')\n",
    "    all_tr = soup.find_all('tr')\n",
    "    all_cities = list()\n",
    "    data = list()\n",
    "    for city in all_tr:\n",
    "        try:\n",
    "            if city['class'][0] == 'rT' or city['class'][0] == 'rS' or city['class'][0] == 'rB':\n",
    "                name = city.select('a')\n",
    "                name = str(name[0].contents[0]).split(\",\")\n",
    "                all_cities.append(name[0])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    for i in range(0, len(all_cities)): #this needs to be set to range(0, len(all_cities)) to run over all cities\n",
    "        population = scrapernoscraping(all_cities[i])\n",
    "        print(all_cities[i])\n",
    "        time.sleep(5)\n",
    "        data.append(population)\n",
    "    output = list(zip(all_cities, data))\n",
    "    with open('output.py', 'w') as file:\n",
    "        file.write(str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for grabbing certain types of data from City-Data website\n",
    "def zipscrape(zips):\n",
    "    for zip in zips: #this method is not completed yet, waiting for my prior run to finish still\n",
    "        print(zips)\n",
    "    return zipout #returns city zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citypopulation(all_sections_cp):\n",
    "    for sect in all_sections_cp:\n",
    "        try:\n",
    "            tup1 = str(sect.contents[1].split(\".\")[0]) #population\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return tup1 #returns city population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def racebr(race):\n",
    "    tup8 = list()\n",
    "    for sect in race:\n",
    "        for i in range(0, len(sect.select('li')[1].contents[0]), 2):\n",
    "            try:\n",
    "                tup8.append((sect.select('li')[1].contents[0].contents[i].contents[2].contents, sect.select('li')[1].contents[0].contents[i].contents[0].contents)) #racial breakdown\n",
    "            except(KeyError, AttributeError):\n",
    "                pass\n",
    "    return tup8 #returns city breakdown by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def men_women(pbs):\n",
    "    for sect in pbs:\n",
    "        try:\n",
    "            tup4 = str(sect.select('td')[1].contents[-1]) #% male in city\n",
    "            tup5 = str(sect.select('td')[3].contents[-1]) #% female in city\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "    return tup4, tup5    #returns city % of men and women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneystuff(ms):\n",
    "    for sect in ms:\n",
    "        try:\n",
    "            tup3 = str(sect.contents[1].strip()) #median-household-income\n",
    "            tup6 = str(sect.select('br')[1].contents[1]) #per capita income\n",
    "            try:\n",
    "                tup7 = str(sect.select('br')[3].contents[1].contents[3]) #median house value\n",
    "            except:\n",
    "                tup7 = str(sect.select('br')[1].contents[10])\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "    return tup3, tup6, tup7 #returns a tuple of median household income, per capita income, median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianage(mage):\n",
    "    for sect in mage:\n",
    "        try:\n",
    "            tup2 = str(sect.select('td')[1].contents[-1]) #median-age info\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return tup2 #returns city median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapernoscraping(name):\n",
    "    namearr = name.split(\" \")\n",
    "    url = 'http://www.city-data.com/city/'\n",
    "    for k in range(0, len(namearr)):\n",
    "        if k == len(namearr) - 1:\n",
    "            url = url + namearr[k]\n",
    "        else:\n",
    "            url = url + namearr[k] + \"-\"\n",
    "    url = url + '-Pennsylvania.html'\n",
    "    web_input = requests.get(url, stream=True)\n",
    "    more_soup = BeautifulSoup(web_input.content, 'html.parser')\n",
    "    all_sections_cp = more_soup.find_all('section', {'id': 'city-population'})\n",
    "    all_section_ma = more_soup.find_all('section', {'class': 'median-age'})\n",
    "    all_section_mhi = more_soup.find_all('section', {'class', 'median-income'})\n",
    "    all_section_pbs = more_soup.find_all('section', {'class', 'population-by-sex'})\n",
    "    all_section_zip = more_soup.find_all('section', {'id': 'zip-codes'})\n",
    "    all_ul_race = more_soup.find_all('ul', {'class': 'list-group'})\n",
    "    #NEED TO GET ZIP CODE SO THAT WE CAN ACTUALLY MERGE OUR DATA\n",
    "    zipscrape(all_section_zip)\n",
    "    #I want to try and make a single FOR loop that takes in a list of Beautiful Soup objects to loop over\n",
    "    tup1 = citypopulation(all_sections_cp)\n",
    "    #for sect in all_sections_cp:\n",
    "     #   try:\n",
    "      #      tup1 = str(sect.contents[1].split(\".\")[0]) #population\n",
    "       # except KeyError:\n",
    "        #    pass\n",
    "    tup2 = medianage(all_section_ma)\n",
    "    #for sect in all_section_ma:\n",
    "     #   try:\n",
    "      #      tup2 = str(sect.select('td')[1].contents[-1]) #median-age info\n",
    "       # except KeyError:\n",
    "        #    pass\n",
    "    tup3, tup6, tup7 = moneystuff(all_section_mhi)\n",
    "    #for sect in all_section_mhi:\n",
    "     #   try:\n",
    "      #      tup3 = str(sect.contents[1].strip()) #median-household-income\n",
    "       #     tup6 = str(sect.select('br')[1].contents[1]) #per capita income\n",
    "        #    try:\n",
    "         #       tup7 = str(sect.select('br')[3].contents[1].contents[3]) #median house value\n",
    "          #  except:\n",
    "           #     tup7 = str(sect.select('br')[1].contents[10])\n",
    "        #except (KeyError, IndexError):\n",
    "         #   pass\n",
    "    tup4, tup5 = men_women(all_section_pbs)\n",
    "    #for sect in all_section_pbs:\n",
    "     #   try:\n",
    "      #      tup4 = str(sect.select('td')[1].contents[-1]) #% male in county\n",
    "       #     tup5 = str(sect.select('td')[3].contents[-1]) #% female in county\n",
    "        #except (KeyError, IndexError):\n",
    "          #  pass\n",
    "    tup8 = racebr(all_ul_race)\n",
    "    #tup8 = list()\n",
    "    #for sect in all_ul_race:\n",
    "     #   for i in range(0, len(sect.select('li')[1].contents[0]), 2):\n",
    "      #      try:\n",
    "       #         tup8.append((sect.select('li')[1].contents[0].contents[i].contents[2].contents, sect.select('li')[1].contents[0].contents[i].contents[0].contents)) #racial breakdown\n",
    "        #    except(KeyError, AttributeError):\n",
    "         #       pass\n",
    "    #Tuple structure is (population, median age, median household income, % of men, % of women, per capita income, median house value, racial breakdown)\n",
    "    tup9 = zipscrape(all_section_zip)\n",
    "    tup = tuple((tup1, tup2, tup3, tup4, tup5, tup6, tup7, tup8, tup9))\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aaronsburg', (' 613', '\\xa052.2 years', '$60,710 ('))]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main('http://www.city-data.com/city/Pennsylvania.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
