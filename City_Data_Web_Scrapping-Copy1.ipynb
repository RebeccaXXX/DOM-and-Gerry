{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed for the methods in this py file.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main method that does the entire web scraping and then saving of the content to a file\n",
    "def main(url, loop=0):\n",
    "    timeout = 60\n",
    "    # Had some blacklist issues so added in this retry loop to try and help\n",
    "    while True:\n",
    "        try:\n",
    "            web_input = requests.get(url, stream=True, timeout=timeout)\n",
    "            break  # If it downloads, get out and get on with the scraping\n",
    "        except requests.exceptions.RequestException as e:  # If it doesn't download after timeout, throw exception and try again\n",
    "            print(e)\n",
    "            pass\n",
    "    soup = BeautifulSoup(web_input.content, 'html.parser')\n",
    "    # Collect all cities from the website that we need to scrape\n",
    "    all_cities = allcities(soup)\n",
    "    data = list()\n",
    "    # This is the main loop that goes through the city names and grabs the data from the individual web pages\n",
    "    for i in range(loop, len(all_cities)): # this needs to be set to range(0, len(all_cities)) to run over all cities\n",
    "        population = scrapernoscraping(all_cities[i])\n",
    "        print(str(i) + \": \" + str(all_cities[i]))\n",
    "        time.sleep(1)\n",
    "        data.append(population)\n",
    "        # Save off files every 150 web pages due to some of the connection issues I was having\n",
    "        if i > 0 and i % 150 == 0:\n",
    "            # Added in loop variable to make sure we start at the zip starting with the correct city\n",
    "            if loop != 0:\n",
    "                output = list(zip(all_cities[loop:], data))\n",
    "            else:\n",
    "                output = list(zip(all_cities, data))\n",
    "            file = 'output' + str(i) + '.py'\n",
    "            # Write statement that creates/updates the file with collected info\n",
    "            with open(file, 'w') as f:\n",
    "                for j in range(0, len(output)):\n",
    "                    f.write(str(output[j]) + \"\\n\")\n",
    "    # Final file creation step that adds the remaining cities found to a file\n",
    "    if loop != 0:\n",
    "        output = list(zip(all_cities[loop:], data))\n",
    "    else:\n",
    "        output = list(zip(all_cities, data))\n",
    "    with open('output.py', 'w') as f:\n",
    "        for k in range(0, len(output)):\n",
    "            f.write(str(output[k]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to collect all cities from the website that we need to scrape\n",
    "def allcities(soup):\n",
    "    all_tr = soup.find_all('tr')\n",
    "    all_cities = list()\n",
    "    # Loop over all cities from the website that we need to scrape\n",
    "    for city in all_tr:\n",
    "        try:\n",
    "            if city['class'][0] == 'rT' or city['class'][0] == 'rS' or city['class'][0] == 'rB':\n",
    "                name = city.select('a')\n",
    "                name = str(name[0].contents[0]).split(\",\")\n",
    "                all_cities.append(name[0])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return all_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to be able to rerun main method with specific start parameter\n",
    "def rerun(line):\n",
    "    main('http://www.city-data.com/city/Pennsylvania.html', loop=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to load in data file of city info\n",
    "def loadindata(file):\n",
    "    arr = list()\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            arr.append(line)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for grabbing certain types of data from City-Data website, the comment after the return statement\n",
    "# tells us what each method returns\n",
    "def zipscrape(zips):\n",
    "    for zip in zips:\n",
    "        try:\n",
    "            zip2 = zip.select('li')\n",
    "            tup9 = zip2[2].contents[0].contents[0]\n",
    "        except:\n",
    "            pass\n",
    "    return tup9 #returns city zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citypopulation(all_sections_cp):\n",
    "    for sect in all_sections_cp:\n",
    "        try:\n",
    "            tup1 = str(sect.contents[1].split(\".\")[0]) #population\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return tup1 #returns city population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def racebr(race):\n",
    "    tup8 = list()\n",
    "    for sect in race:\n",
    "        for i in range(0, len(sect.select('li')[1].contents[0]), 2):\n",
    "            try:\n",
    "                tup8.append((sect.select('li')[1].contents[0].contents[i].contents[2].contents, sect.select('li')[1].contents[0].contents[i].contents[0].contents)) #racial breakdown\n",
    "            except(KeyError, AttributeError):\n",
    "                pass\n",
    "    return tup8 #returns city breakdown by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def men_women(pbs):\n",
    "    for sect in pbs:\n",
    "        try:\n",
    "            tup4 = str(sect.select('td')[1].contents[-1]) #% male in city\n",
    "            tup5 = str(sect.select('td')[3].contents[-1]) #% female in city\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "    return tup4, tup5    #returns city % of men and women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneystuff(ms):\n",
    "    for sect in ms:\n",
    "        try:\n",
    "            tup3 = str(sect.contents[1].strip()) #median-household-income\n",
    "            tup6 = str(sect.select('br')[1].contents[1]) #per capita income\n",
    "            try:\n",
    "                tup7 = str(sect.select('br')[3].contents[1].contents[3]) #median house value\n",
    "            except:\n",
    "                tup7 = str(sect.select('br')[1].contents[10])\n",
    "        except (KeyError, IndexError):\n",
    "            pass\n",
    "    return tup3, tup6, tup7 #returns a tuple of median household income, per capita income, median house value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianage(mage):\n",
    "    for sect in mage:\n",
    "        try:\n",
    "            tup2 = str(sect.select('td')[1].contents[-1]) #median-age info\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return tup2 #returns city median age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that does the individual web page scraping for all the data we were collecting\n",
    "def scrapernoscraping(name):\n",
    "    # Special handling for cities with spaces in their names\n",
    "    namearr = name.split(\" \")\n",
    "    url = 'http://www.city-data.com/city/'\n",
    "    for k in range(0, len(namearr)):\n",
    "        if k == len(namearr) - 1:\n",
    "            url = url + namearr[k]\n",
    "        else:\n",
    "            url = url + namearr[k] + \"-\"\n",
    "    url = url + '-Pennsylvania.html'\n",
    "    # Special handling for some weirdly named cities\n",
    "    if name == \"O'Hara Township\":\n",
    "        url = \"http://www.city-data.com/city/O-Hara-Township-Pennsylvania.html\"\n",
    "    elif name == 'Penn State Erie (Behrend)':\n",
    "        url = 'http://www.city-data.com/city/Penn-State-Erie-Behrend-Pennsylvania.html'\n",
    "    elif name == 'Tharptown (Uniontown)':\n",
    "        url = 'http://www.city-data.com/city/Tharptown-Uniontown-Pennsylvania.html'\n",
    "\n",
    "    # Initialize all core variables for use with the Beautiful Soup object\n",
    "    timeout = 60\n",
    "    web_input = requests.get(url, stream=True, timeout=timeout)\n",
    "    more_soup = BeautifulSoup(web_input.content, 'html.parser')\n",
    "    all_sections_cp = more_soup.find_all('section', {'id': 'city-population'})\n",
    "    all_section_ma = more_soup.find_all('section', {'class': 'median-age'})\n",
    "    all_section_mhi = more_soup.find_all('section', {'class', 'median-income'})\n",
    "    all_section_pbs = more_soup.find_all('section', {'class', 'population-by-sex'})\n",
    "    all_ol_county = more_soup.find_all('ol', {'class': 'breadcrumb'})\n",
    "    all_ul_race = more_soup.find_all('ul', {'class': 'list-group'})\n",
    "\n",
    "    # Grab each data point that we are looking for from the website\n",
    "    tup1 = citypopulation(all_sections_cp)\n",
    "    tup2 = medianage(all_section_ma)\n",
    "    tup3, tup6, tup7 = moneystuff(all_section_mhi)\n",
    "    tup4, tup5 = men_women(all_section_pbs)\n",
    "    tup8 = racebr(all_ul_race)\n",
    "    tup9 = zipscrape(all_ol_county)\n",
    "    # Structure is (population, median age, median household income, % of men, % of women, per capita income, median house value, racial breakdown, county)\n",
    "    metrics = tup1 + ' | ' + tup2 + ' | ' + tup3 + ' | ' + tup4 + ' | ' + tup5 + ' | ' + tup6 + ' | ' + tup7 + ' | ' + str(tup8) + ' | ' + tup9\n",
    "    return metrics #Altered the return to make things easier for me to parse and clean later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aaronsburg', (' 613', '\\xa052.2 years', '$60,710 ('))]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #scrapernoscraping(\"Wyalusing\")  # Way for me to test individual cities that may have had issues while scraping\n",
    "    main('http://www.city-data.com/city/Pennsylvania.html')\n",
    "    # rerun(1651)  # Rerun tag so I could restart my scraping if it lost connection to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
